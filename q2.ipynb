{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "    def transfer(self, activation):\n",
    "        '''\n",
    "        Tanh activation function.\n",
    "        '''\n",
    "        return (exp(activation) - exp(-activation))/(exp(activation) + exp(-activation))\n",
    "    \n",
    "    def transfer_derivative(self, output):\n",
    "        '''\n",
    "        We are using the tanh transfer function, the derivative of which can be calculated as follows:\n",
    "        derivative = 1 - (output * output)\n",
    "        '''\n",
    "        return 1 - (output * output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\n",
    "    def transfer(self, activation):\n",
    "        '''\n",
    "        Sigmoid activation function.\n",
    "        '''\n",
    "        return 1.0 / (1.0 + exp(-activation * 1.0))\n",
    "    \n",
    "    def transfer_derivative(self, output):\n",
    "        '''\n",
    "        We are using the sigmoid transfer function, the derivative of which can be calculated as follows:\n",
    "        derivative = output * (1.0 - output)\n",
    "        '''\n",
    "        return output * (1.0 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('dermatology.csv', sep=',',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[((df[34]==1) | (df[34]==2) | (df[34]==3)) & (df[33]!='?')]\n",
    "df[34]-=1\n",
    "dataset = df.values.astype('float') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp = MultilayerNnClassifier()\n",
    "activationFunction = Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_in = dataset[:,:-1]\n",
    "dataset_out = dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satyam/.local/lib/python2.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "training_rate = 0.5\n",
    "weights = train_network_main(dataset_in.T, dataset_out.T, training_rate, shape, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.94086170e+00  6.95203702e+04  6.94707247e+04  6.95106215e+04\n",
      "   6.94234527e+04  6.95199800e+04  6.94315231e+04  1.60657337e+01\n",
      "   6.94024972e+04  6.94479523e+04  6.95022502e+04 -4.95368030e+00\n",
      "   6.92850578e+04  6.93953063e+04  6.93557800e+04  6.94270365e+04\n",
      "   6.94531880e+04  6.94758623e+04  6.94757342e+04  6.94700239e+04\n",
      "   6.95051767e+04  6.94396464e+04  6.94989529e+04 -1.04207957e+01\n",
      "  -3.08198361e+00  6.94016699e+04  6.94885936e+04  6.94106000e+04\n",
      "   6.93392372e+04  6.95106037e+04 -1.18070331e+01  6.95066854e+04\n",
      "   6.94493330e+04  1.50730511e+01  6.94948511e+04  6.95107435e+04\n",
      "   6.95121797e+04  6.95088958e+04  6.94354794e+04  6.94636776e+04]\n",
      " [-2.98819154e+00  6.95186699e+04  6.94877299e+04  6.95109310e+04\n",
      "   6.94042011e+04  6.95050691e+04  6.94488206e+04 -1.00336556e+01\n",
      "   6.93821393e+04  6.94258984e+04  6.94903314e+04 -1.59839838e+00\n",
      "   6.92800540e+04  6.93908432e+04  6.93614453e+04  6.94327835e+04\n",
      "   6.94654591e+04  6.94756133e+04  6.94915735e+04  6.94529525e+04\n",
      "   6.94881693e+04  6.94447633e+04  6.95119793e+04 -2.43273627e+00\n",
      "  -3.12115389e-01  6.93972977e+04  6.94725073e+04  6.93933757e+04\n",
      "   6.93156951e+04  6.94984683e+04 -4.87286126e-01  6.94983863e+04\n",
      "   6.94599535e+04 -9.17573427e+00  6.95102328e+04  6.94964641e+04\n",
      "   6.95112212e+04  6.94919420e+04  6.94453937e+04  6.94438949e+04]\n",
      " [ 1.40698330e+00  6.95100959e+04  6.94706214e+04  6.95023349e+04\n",
      "   6.94185984e+04  6.95051072e+04  6.94315577e+04 -1.03241756e+01\n",
      "   6.93915044e+04  6.94386333e+04  6.94973586e+04 -3.23370245e+00\n",
      "   6.92895369e+04  6.94077579e+04  6.93293480e+04  6.94001440e+04\n",
      "   6.94405638e+04  6.94783599e+04  6.94876184e+04  6.94649595e+04\n",
      "   6.95012625e+04  6.94375554e+04  6.94933171e+04 -4.96463607e-01\n",
      "  -1.49571053e+00  6.94142651e+04  6.94838798e+04  6.94044782e+04\n",
      "   6.93232946e+04  6.95048559e+04  6.52152835e-01  6.94964607e+04\n",
      "   6.94616831e+04 -1.03140718e+01  6.94887705e+04  6.95058790e+04\n",
      "   6.95023078e+04  6.95049585e+04  6.94486238e+04  6.94584163e+04]]\n"
     ]
    }
   ],
   "source": [
    "print weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 34)\n",
      "(3, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satyam/.local/lib/python2.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: overflow encountered in exp\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "test_input = dataset_in.T\n",
    "test_output = run_network(test_input, shape, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, inputl, hidden, outputl):\n",
    "        self.hidden_nodes = hidden\n",
    "        self.num_features = inputl\n",
    "        self.num_labels = outputl\n",
    "        np.random.seed(0)\n",
    "        self.layer1_weights_array = np.random.normal(0.01, 1, [self.num_features, self.hidden_nodes])\n",
    "        self.layer1_biases_array = np.zeros((1, self.hidden_nodes))\n",
    "        self.layer2_weights_array = np.random.normal(0.01, 1, [self.hidden_nodes, self.num_labels])\n",
    "        self.layer2_biases_array = np.zeros((1, self.num_labels))\n",
    "\n",
    "    def relu_activation(self, data_array):\n",
    "        return np.maximum(data_array, 0)\n",
    "    \n",
    "    def softmax(self, output_array):\n",
    "        logits_exp = np.exp(output_array.astype(np.float)-np.max(output_array))\n",
    "        val = logits_exp / np.nansum(logits_exp, axis = 1, keepdims = True)\n",
    "        val[np.isnan(val)] = 0\n",
    "        return val\n",
    "    \n",
    "    def cross_entropy_softmax_loss_array(self, softmax_probs_array, y_onehot):\n",
    "        indices = np.argmax(y_onehot, axis = 1).astype(int)\n",
    "        predicted_probability = softmax_probs_array[np.arange(len(softmax_probs_array)), indices]\n",
    "        log_preds = np.log(predicted_probability+sys.float_info.epsilon)\n",
    "        log_preds[np.isnan(log_preds)] = 0\n",
    "        loss = (-1.0 * np.nansum(log_preds)) / (1.0 * len(log_preds) )\n",
    "        return loss\n",
    "    \n",
    "    def get_labels_in_one_hot_encoding(self, labels):\n",
    "        labels_onehot = np.zeros((labels.shape[0], self.num_labels)).astype(int)\n",
    "        labels_onehot[np.arange(len(labels)), labels.astype(int)] = 1\n",
    "        return labels_onehot\n",
    "    \n",
    "\n",
    "    def regularization_L2_softmax_loss(self, reg_lambda, weight1, weight2):\n",
    "        weight1_loss = 0.5 * reg_lambda * np.nansum(weight1 * weight1)\n",
    "        weight2_loss = 0.5 * reg_lambda * np.nansum(weight2 * weight2)\n",
    "        return weight1_loss + weight2_loss\n",
    "    \n",
    "    \n",
    "    def train(self, data, labels, reg_lambda, learning_rate):\n",
    "        pr_loss = 0\n",
    "        delta_loss = 100\n",
    "        loss = 100\n",
    "        delta = 0.0001\n",
    "        step = 0\n",
    "        while loss>delta and delta_loss>0.01* delta:\n",
    "            input_layer = np.dot(data, self.layer1_weights_array)\n",
    "            hidden_layer = self.relu_activation(input_layer + self.layer1_biases_array)\n",
    "            output_layer = np.dot(hidden_layer, self.layer2_weights_array) + self.layer2_biases_array\n",
    "            output_probs = self.softmax(output_layer)\n",
    "            loss = self.cross_entropy_softmax_loss_array(output_probs, labels)\n",
    "            loss += self.regularization_L2_softmax_loss(reg_lambda, self.layer1_weights_array, self.layer2_weights_array)\n",
    "\n",
    "            output_error_signal = (output_probs - labels) / output_probs.shape[0]\n",
    "\n",
    "            error_signal_hidden = np.dot(output_error_signal, self.layer2_weights_array.T) \n",
    "            error_signal_hidden[hidden_layer <= 0] = 0\n",
    "\n",
    "            gradient_layer2_weights = np.dot(hidden_layer.T, output_error_signal)\n",
    "            gradient_layer2_bias = np.sum(output_error_signal, axis = 0, keepdims = True)\n",
    "\n",
    "            gradient_layer1_weights = np.dot(data.T, error_signal_hidden)\n",
    "            gradient_layer1_bias = np.sum(error_signal_hidden, axis = 0, keepdims = True)\n",
    "\n",
    "            gradient_layer2_weights += reg_lambda * self.layer2_weights_array\n",
    "            gradient_layer1_weights += reg_lambda * self.layer1_weights_array\n",
    "\n",
    "            self.layer1_weights_array -= learning_rate * gradient_layer1_weights\n",
    "            self.layer1_biases_array -= learning_rate * gradient_layer1_bias\n",
    "            self.layer2_weights_array -= learning_rate * gradient_layer2_weights\n",
    "            self.layer2_biases_array -= learning_rate * gradient_layer2_bias\n",
    "            delta_loss = abs(loss-pr_loss)\n",
    "            pr_loss = loss\n",
    "            if step % 500 == 0:\n",
    "                    print 'Loss at step {0}: {1}'.format(step, loss)\n",
    "            step += 1\n",
    "\n",
    "    def predict(self, test_dataset, test_labels):\n",
    "        input_layer = np.dot(test_dataset, self.layer1_weights_array)\n",
    "        hidden_layer = self.relu_activation(input_layer + self.layer1_biases_array)\n",
    "        scores = np.dot(hidden_layer, self.layer2_weights_array) + self.layer2_biases_array\n",
    "        probs = self.softmax(scores)\n",
    "        print self.accuracy(probs, test_labels)\n",
    "        \n",
    "    def accuracy(self, predictions, labels):\n",
    "        preds_correct_boolean =  np.argmax(predictions, 1) == np.argmax(labels, 1)\n",
    "        correct_predictions = np.sum(preds_correct_boolean)\n",
    "        accuracy = (100.0 * correct_predictions) / predictions.shape[0]\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = NeuralNetwork(34,4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 20.6698861333\n",
      "Loss at step 500: 1.16486716906\n",
      "Loss at step 1000: 0.681321762138\n",
      "Loss at step 1500: 0.463562725132\n",
      "Loss at step 2000: 0.625790770447\n",
      "Loss at step 2500: 0.542061464103\n",
      "Loss at step 3000: 0.594901781216\n",
      "Loss at step 3500: 0.727963411366\n",
      "Loss at step 4000: 0.553483144232\n",
      "Loss at step 4500: 0.816272765552\n",
      "Loss at step 5000: 0.678383194249\n",
      "Loss at step 5500: 0.589022736733\n",
      "Loss at step 6000: 1.78341279891\n",
      "Loss at step 6500: 0.731774074839\n",
      "Loss at step 7000: 0.537608421989\n",
      "Loss at step 7500: 0.699577242802\n",
      "Loss at step 8000: 0.569415401434\n",
      "Loss at step 8500: 1.18068698445\n",
      "Loss at step 9000: 1.21306865478\n",
      "Loss at step 9500: 0.619158980516\n",
      "Loss at step 10000: 0.731162293407\n",
      "Loss at step 10500: 1.07555625911\n",
      "Loss at step 11000: 0.614275986963\n",
      "Loss at step 11500: 0.62374465664\n",
      "Loss at step 12000: 0.627654808342\n",
      "Loss at step 12500: 0.657768977\n",
      "Loss at step 13000: 0.698929993165\n",
      "Loss at step 13500: 0.63717952531\n",
      "Loss at step 14000: 0.761015479189\n",
      "Loss at step 14500: 0.795333361184\n",
      "Loss at step 15000: 0.623948390676\n",
      "Loss at step 15500: 0.666473169805\n",
      "Loss at step 16000: 0.677859002341\n",
      "Loss at step 16500: 1.91770264757\n",
      "Loss at step 17000: 0.752537121095\n",
      "Loss at step 17500: 0.598893934598\n",
      "Loss at step 18000: 0.636792299261\n",
      "Loss at step 18500: 0.978368499638\n",
      "Loss at step 19000: 0.76782120221\n",
      "Loss at step 19500: 0.500091899279\n",
      "Loss at step 20000: 1.16417428111\n",
      "Loss at step 20500: 0.754402568699\n",
      "Loss at step 21000: 0.887028366236\n",
      "Loss at step 21500: 0.891356895326\n",
      "Loss at step 22000: 0.508824053627\n",
      "Loss at step 22500: 0.602502460669\n",
      "Loss at step 23000: 0.867364948408\n",
      "Loss at step 23500: 0.522556948206\n",
      "Loss at step 24000: 0.683877886248\n",
      "Loss at step 24500: 1.22807572913\n",
      "Loss at step 25000: 0.772945551136\n",
      "Loss at step 25500: 0.693829116677\n",
      "Loss at step 26000: 0.632070969229\n",
      "Loss at step 26500: 0.727422953549\n",
      "Loss at step 27000: 0.591577088418\n",
      "Loss at step 27500: 1.79786931364\n",
      "Loss at step 28000: 1.08640633502\n",
      "Loss at step 28500: 0.686092259454\n",
      "Loss at step 29000: 0.711443784834\n",
      "Loss at step 29500: 0.705429141459\n",
      "Loss at step 30000: 2.08032683306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-985-64d26e2c74ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_onehot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-981-8c88e4f4af86>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, labels, reg_lambda, learning_rate)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mhidden_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_biases_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0moutput_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2_weights_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2_biases_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0moutput_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_softmax_loss_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization_L2_softmax_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_lambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_weights_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2_weights_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-981-8c88e4f4af86>\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(self, output_array)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlogits_exp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_exp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnansum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_exp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/nanfunctions.pyc\u001b[0m in \u001b[0;36mnansum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \"\"\"\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_replace_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/nanfunctions.pyc\u001b[0m in \u001b[0;36m_replace_nan\u001b[0;34m(a, val)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \"\"\"\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "network.train(dataset_in,labels_onehot,0.1, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_onehot = network.get_labels_in_one_hot_encoding(dataset_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.58617836  0.12278547  0.35278128  0.49988728]\n",
      " [ 0.63855407 -0.36810269  0.34254046 -0.08638118]\n",
      " [-0.06254936  0.13329651  0.05441899  0.51330052]\n",
      " [ 0.25990555  0.03318686  0.1615898  -0.08843037]\n",
      " [ 0.53843244 -0.07552668  0.11483682 -0.23928151]\n",
      " [-0.90899405  0.23721108  0.31256802 -0.45686688]\n",
      " [ 0.81490039 -0.51688534  0.01993093 -0.05768287]\n",
      " [ 0.55146785  0.52879815  0.05896061 -0.04510642]\n",
      " [-0.30658312 -0.71173002 -0.12207506  0.44890636]\n",
      " [ 0.4525712   0.42292906 -0.13616387  0.30867529]\n",
      " [-0.36993281 -0.50564959 -0.60633334  0.76731683]\n",
      " [-0.17860113 -0.15301555 -0.44423835  0.08610573]\n",
      " [-0.57331476 -0.07463318 -0.31651084  0.06333756]\n",
      " [-0.19587171 -0.42762266 -0.00778731  0.2147975 ]\n",
      " [ 0.02735115  0.11169337 -0.22316451 -0.13408409]\n",
      " [-0.27485297 -0.13659145 -0.28708538 -1.15157536]\n",
      " [ 0.0549692  -0.16105787 -0.58042944  0.14020861]\n",
      " [-0.31907488  0.01593869  0.26354458  0.19777152]\n",
      " [ 0.38025197 -0.45179737  0.14674787 -0.11762539]\n",
      " [-0.29786925 -0.21588852 -0.10907828  0.64290044]\n",
      " [-0.40464278  0.31013498  0.16873793  0.08829818]\n",
      " [ 0.54354109  0.66732725  0.42428633  0.55301417]\n",
      " [-0.37789767  0.3735023  -0.14054146  0.58926558]\n",
      " [ 0.08749133  0.34396517  0.12966995  0.57959747]\n",
      " [ 0.00732775  0.64193556  0.04893935 -0.03707077]\n",
      " [ 0.6816314  -0.4832776  -0.45056152  0.63272838]\n",
      " [-0.41575953  0.69821064 -0.14427398 -0.46326966]\n",
      " [ 0.65184916  0.52152063  0.67113518 -0.45934837]\n",
      " [-0.3042714   0.68632899 -0.09222354  0.08646015]\n",
      " [ 0.3421706  -0.05183399  0.22307775  0.32724993]\n",
      " [ 0.13812816 -0.38940732  0.11018002  0.47172417]\n",
      " [-0.27696382 -0.06940039 -0.15197153  0.43540908]\n",
      " [ 0.24538644  0.14882357 -0.27292075 -0.046238  ]\n",
      " [-0.39555997 -0.41831302 -0.22370926 -0.00387768]]\n",
      "[[ 0.16152163  0.03866998  0.08373121]\n",
      " [-0.37603001 -0.52470567  0.14475293]\n",
      " [ 0.06316358  0.23056633  0.85542108]\n",
      " [ 0.90490189 -0.06122102 -0.42236293]]\n",
      "[[8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.96658126e-01 3.09335668e-03 2.48517325e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99883976e-01 1.13438726e-04 2.58491690e-06]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98779377e-01 1.15773223e-03 6.28905121e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98721140e-01 1.21189096e-03 6.69687333e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99998527e-01 1.46650103e-06 6.57696422e-09]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [4.10918993e-01 3.57956125e-01 2.31124882e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98861476e-01 1.08126996e-03 5.72539572e-05]\n",
      " [1.23941130e-01 4.22203060e-01 4.53855810e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98673546e-01 1.25610466e-03 7.03492330e-05]\n",
      " [1.27940321e-01 4.22754619e-01 4.49305059e-01]\n",
      " [9.99801557e-01 1.93075894e-04 5.36739146e-06]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98421557e-01 1.48952483e-03 8.89184827e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.70357770e-01 2.52529726e-02 4.38925717e-03]\n",
      " [1.60681539e-01 4.24438758e-01 4.14879703e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.95141621e-01 4.45708590e-03 4.01293434e-04]\n",
      " [9.97671335e-01 2.17869745e-03 1.49967413e-04]\n",
      " [8.68395305e-01 1.00916897e-01 3.06877977e-02]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.96343296e-01 3.38216351e-03 2.74540719e-04]\n",
      " [9.98672518e-01 1.25705915e-03 7.04227101e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99977523e-01 2.22019302e-05 2.74959079e-07]\n",
      " [9.79779988e-01 1.75642117e-02 2.65580007e-03]\n",
      " [1.21351890e-01 4.21798479e-01 4.56849631e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.85790045e-01 8.86979832e-02 2.55119718e-02]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.69484507e-01 2.59559693e-02 4.55952361e-03]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.39718001e-01 4.91822965e-02 1.10997020e-02]\n",
      " [9.75772736e-01 2.08588124e-02 3.36845130e-03]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98783325e-01 1.15405826e-03 6.26164011e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99917964e-01 8.04245628e-05 1.61150681e-06]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.01718839e-01 7.73014814e-02 2.09796792e-02]\n",
      " [8.51277243e-02 4.11200458e-01 5.03671818e-01]\n",
      " [9.98724429e-01 1.20883390e-03 6.67366807e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.92427501e-01 6.84785472e-03 7.24644460e-04]\n",
      " [9.88390007e-01 1.03328522e-02 1.27714041e-03]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99417451e-01 5.59400367e-04 2.31481577e-05]\n",
      " [9.99590240e-01 3.95390246e-04 1.43701170e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.84203284e-01 1.35995406e-02 2.19717542e-03]\n",
      " [3.23913511e-01 3.90942423e-01 2.85144066e-01]\n",
      " [9.45115410e-01 4.50628146e-02 9.82177580e-03]\n",
      " [9.97251271e-01 2.56139496e-03 1.87333648e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99055466e-01 9.00038174e-04 4.44957472e-05]\n",
      " [9.94022187e-01 5.44875419e-03 5.29058976e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99233385e-01 7.33053079e-04 3.35620512e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.88106667e-01 1.05747907e-02 1.31854234e-03]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.78058582e-01 1.89843003e-02 2.95711810e-03]\n",
      " [9.96993492e-01 2.79526220e-03 2.11245846e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.38578999e-01 5.00471277e-02 1.13738737e-02]\n",
      " [7.70156818e-01 1.66165245e-01 6.36779367e-02]\n",
      " [8.82309589e-01 9.11611125e-02 2.65292987e-02]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [1.49663805e-01 4.24377426e-01 4.25958769e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.97081943e-01 2.71509610e-03 2.02960967e-04]\n",
      " [9.99380902e-01 5.93961938e-04 2.51356769e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99667320e-01 3.21848999e-04 1.08309262e-05]\n",
      " [9.99473895e-01 5.05941222e-04 2.01640697e-05]\n",
      " [9.99419798e-01 5.57179537e-04 2.30219804e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99882434e-01 1.14934441e-04 2.63185647e-06]\n",
      " [9.99941781e-01 5.72096229e-05 1.00928892e-06]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.92182508e-01 7.06153371e-03 7.55958233e-04]\n",
      " [9.42322948e-01 4.71985707e-02 1.04784813e-02]\n",
      " [8.92104781e-01 8.42047288e-02 2.36904900e-02]\n",
      " [9.61039743e-01 3.26822372e-02 6.27801965e-03]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98159267e-01 1.73138558e-03 1.09347145e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99916939e-01 8.14217884e-05 1.63902213e-06]\n",
      " [9.99990837e-01 9.08208503e-06 8.05281210e-08]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [3.60053614e-01 3.78125566e-01 2.61820820e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99684019e-01 3.05881701e-04 1.00995871e-05]\n",
      " [9.98588469e-01 1.33503620e-03 7.64951670e-05]\n",
      " [9.98871544e-01 1.07188437e-03 5.65721092e-05]\n",
      " [9.99659931e-01 3.28909831e-04 1.11587228e-05]\n",
      " [9.99716477e-01 2.74805541e-04 8.71721824e-06]\n",
      " [9.99756517e-01 2.36394646e-04 7.08828613e-06]\n",
      " [9.99059854e-01 8.95928756e-04 4.42168108e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.57177188e-02 4.15412405e-01 4.88869876e-01]\n",
      " [9.96526718e-01 3.21700676e-03 2.56275299e-04]\n",
      " [9.89189453e-01 9.64850270e-03 1.16204391e-03]\n",
      " [9.40764745e-01 4.83861595e-02 1.08490956e-02]\n",
      " [9.97112174e-01 2.68767757e-03 2.00148258e-04]\n",
      " [9.26702323e-01 5.89777577e-02 1.43199197e-02]\n",
      " [8.28638284e-01 1.28047193e-01 4.33145231e-02]\n",
      " [9.99741557e-01 2.50756465e-04 7.68657748e-06]\n",
      " [9.99065553e-01 8.90591379e-04 4.38552387e-05]\n",
      " [9.99597633e-01 3.88347740e-04 1.40196225e-05]\n",
      " [9.99954924e-01 4.43641454e-05 7.11695927e-07]\n",
      " [9.99965022e-01 3.44749216e-05 5.03293758e-07]\n",
      " [9.99761220e-01 2.31877168e-04 6.90285054e-06]\n",
      " [9.99378022e-01 5.96684037e-04 2.52940959e-05]\n",
      " [9.98541338e-01 1.37870730e-03 7.99550928e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99510608e-01 4.71110035e-04 1.82816287e-05]\n",
      " [9.99795593e-01 1.98818933e-04 5.58794790e-06]\n",
      " [9.99870752e-01 1.26253350e-04 2.99436930e-06]\n",
      " [9.99655269e-01 3.33364135e-04 1.13668740e-05]\n",
      " [9.99760395e-01 2.32669663e-04 6.93528434e-06]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.63652858e-02 4.15635549e-01 4.87999165e-01]\n",
      " [9.98486050e-01 1.42988769e-03 8.40624911e-05]\n",
      " [9.99292740e-01 6.77162839e-04 3.00969045e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [1.69018280e-01 4.24194584e-01 4.06787136e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.98249929e-01 1.64790492e-03 1.02166515e-04]\n",
      " [9.99593293e-01 3.92482341e-04 1.42251096e-05]\n",
      " [9.99922754e-01 7.57617948e-05 1.48455735e-06]\n",
      " [9.97902956e-01 1.96675749e-03 1.30286347e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99890034e-01 1.07563691e-04 2.40279479e-06]\n",
      " [9.98343082e-01 1.56199938e-03 9.49184590e-05]\n",
      " [9.98060595e-01 1.82210627e-03 1.17299017e-04]\n",
      " [9.98795187e-01 1.14301836e-03 6.17946939e-05]\n",
      " [9.97310660e-01 2.50741318e-03 1.81927159e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99837351e-01 1.58553866e-04 4.09474385e-06]\n",
      " [9.99467634e-01 5.11876342e-04 2.04897907e-05]\n",
      " [9.99894638e-01 1.03095062e-04 2.26672941e-06]\n",
      " [9.98135551e-01 1.75320380e-03 1.11245613e-04]\n",
      " [9.99730432e-01 2.61428718e-04 8.13958531e-06]\n",
      " [9.99446115e-01 5.32265118e-04 2.16194582e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99313316e-01 6.57765436e-04 2.89186638e-05]\n",
      " [9.98837692e-01 1.10343468e-03 5.88729807e-05]\n",
      " [9.98621171e-01 1.30471159e-03 7.41173928e-05]\n",
      " [9.98205178e-01 1.68912644e-03 1.05695574e-04]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99870602e-01 1.26399347e-04 2.99912747e-06]\n",
      " [9.99415420e-01 5.61322390e-04 2.32575096e-05]\n",
      " [9.99875118e-01 1.21372267e-04 3.50998107e-06]\n",
      " [9.99398968e-01 5.76883545e-04 2.41479851e-05]\n",
      " [9.99519914e-01 4.62273751e-04 1.78121525e-05]\n",
      " [9.99518918e-01 4.63219928e-04 1.78622641e-05]\n",
      " [9.99049202e-01 9.05903237e-04 4.48946783e-05]\n",
      " [9.99485261e-01 4.95162850e-04 1.95762049e-05]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [8.09059315e-02 4.09193234e-01 5.09900835e-01]\n",
      " [9.99958619e-01 4.07481371e-05 6.33238039e-07]]\n",
      "76.8595041322\n"
     ]
    }
   ],
   "source": [
    "network.predict(dataset_in, labels_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  2.  0.  3.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  1.  0. 55.]\n",
      "[3. 3. 3. 2. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 2. 0. 2. 2. 2. 2. 2. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 8.]\n",
      "[ 2.  1.  2.  3.  1.  3.  0.  3.  0.  0.  0.  1.  0.  0.  0.  1.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  2.  3.  2.  0.  0.  2.  3. 26.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  3.  2.  0.  0.  0.  3.  0.  0.  2.  0.\n",
      "  3.  2.  2.  2.  2.  0.  0.  3.  0.  0.  0.  0.  0.  3.  0. 40.]\n",
      "[ 2.  3.  2.  2.  2.  2.  0.  2.  0.  0.  0.  1.  0.  0.  0.  1.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  2.  2.  3.  2.  3.  0.  0.  2.  3. 45.]\n",
      "[ 2.  3.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  1.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 41.]\n",
      "[ 2.  2.  3.  3.  3.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  2.  2.  3.  2.  0.  0.  3.  3. 57.]\n",
      "[ 3.  3.  2.  1.  1.  0.  0.  0.  2.  2.  1.  0.  0.  0.  0.  0.  3.  2.\n",
      "  3.  2.  2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 20.]\n",
      "[ 2.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  2.  2.  0.\n",
      "  0.  0.  0.  0.  1.  0.  0.  0.  0.  3.  0.  0.  0.  1.  0. 21.]\n",
      "[ 3.  3.  1.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.  0.  3.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 22.]\n",
      "[ 2.  3.  3.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  1.  0.  0.  2.  1.\n",
      "  2.  1.  2.  3.  0.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0. 10.]\n",
      "[ 2.  2.  3.  3.  0.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  1.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  0.  3.  0.  0.  1.  3. 65.]\n",
      "[ 2.  2.  1.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  2.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 30.]\n",
      "[ 3.  3.  3.  0.  0.  0.  0.  0.  3.  3.  1.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  3.  3.  3.  2.  3.  0.  3.  0.  0.  0.  0.  0.  2.  0. 38.]\n",
      "[ 2.  1.  3.  3.  3.  3.  0.  0.  2.  0.  0.  3.  0.  0.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  3.  0.  2.  0.  3.  0.  0.  2.  3. 23.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  2.  2.  1.\n",
      "  2.  0.  2.  1.  2.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 51.]\n",
      "[ 2.  2.  2.  3.  2.  2.  0.  2.  0.  0.  0.  3.  2.  0.  0.  0.  2.  1.\n",
      "  1.  0.  0.  0.  0.  0.  3.  0.  3.  0.  2.  0.  0.  2.  3. 44.]\n",
      "[ 2.  1.  1.  0.  1.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  2.  2.\n",
      "  2.  2.  2.  2.  1.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 33.]\n",
      "[ 3.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  3.  3.  3.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  1.  1.  2.  0. 43.]\n",
      "[ 1.  1.  2.  3.  2.  2.  0.  3.  0.  0.  0.  2.  0.  0.  0.  2.  2.  1.\n",
      "  2.  0.  0.  0.  0.  0.  3.  0.  3.  0.  3.  1.  0.  2.  3. 50.]\n",
      "[ 3.  2.  1.  2.  0.  0.  0.  0.  1.  2.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  3.  2.  2.  2.  1.  2.  0.  2.  0.  0.  0.  0.  0.  1.  0. 50.]\n",
      "[ 3.  2.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  2.  0.  2.  1.  1.\n",
      "  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 10.]\n",
      "[ 2.  3.  3.  3.  3.  0.  0.  0.  3.  3.  0.  0.  0.  0.  0.  0.  3.  2.\n",
      "  2.  3.  3.  3.  1.  3.  0.  0.  0.  0.  0.  0.  0.  1.  0. 34.]\n",
      "[ 2.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  3.  2.  1.\n",
      "  0.  0.  0.  0.  2.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 15.]\n",
      "[ 2.  1.  2.  3.  2.  1.  0.  2.  0.  0.  0.  0.  0.  0.  0.  2.  2.  2.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  1.  0.  3.  0.  0.  2.  3. 26.]\n",
      "[ 3.  3.  2.  0.  0.  0.  0.  0.  2.  2.  1.  0.  0.  1.  0.  0.  2.  2.\n",
      "  3.  2.  2.  1.  0.  2.  0.  0.  0.  0.  0.  0.  0.  1.  0. 46.]\n",
      "[ 1.  1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  2.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  1.  0. 51.]\n",
      "[ 3.  2.  1.  1.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.  0.  0.  2.  1.\n",
      "  1.  1.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 15.]\n",
      "[ 2.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  1.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 35.]\n",
      "[ 2.  1.  1.  1.  1.  2.  0.  1.  0.  0.  0.  2.  0.  0.  0.  3.  2.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  0.  2.  0.  0.  3.  3. 48.]\n",
      "[ 1.  2.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  2.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  2. 52.]\n",
      "[ 2.  0.  1.  0.  0.  2.  0.  1.  0.  0.  0.  3.  0.  0.  0.  2.  2.  2.\n",
      "  2.  0.  0.  0.  0.  0.  1.  0.  3.  0.  2.  0.  0.  2.  2. 60.]\n",
      "[ 3.  1.  1.  2.  2.  2.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  2.  0.  3.  0.  0.  2.  3. 32.]\n",
      "[ 2.  2.  1.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  1.  0.  0.  2.  1.\n",
      "  1.  1.  2.  2.  1.  2.  0.  0.  0.  0.  0.  0.  0.  1.  0. 41.]\n",
      "[ 3.  1.  1.  1.  1.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.\n",
      "  2.  2.  2.  3.  0.  3.  0.  0.  0.  0.  0.  0.  0.  2.  0. 48.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  2.  0.\n",
      "  2.  1.  3.  2.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 19.]\n",
      "[ 1.  1.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  3.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 22.]\n",
      "[ 2.  1.  1.  3.  0.  3.  0.  1.  0.  0.  0.  1.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  3.  0.  1.  0.  0.  2.  2. 29.]\n",
      "[ 3.  3.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  3.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 33.]\n",
      "[ 2.  1.  1.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  1.  0.  0.  2.  2.\n",
      "  2.  2.  2.  2.  0.  1.  0.  1.  0.  0.  0.  0.  0.  1.  0. 40.]\n",
      "[ 3.  3.  2.  1.  1.  0.  0.  0.  2.  2.  1.  0.  0.  0.  0.  0.  3.  2.\n",
      "  3.  2.  2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 42.]\n",
      "[ 2.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  3.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 36.]\n",
      "[ 3.  2.  2.  2.  0.  2.  0.  1.  0.  0.  0.  2.  0.  0.  0.  1.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  0.  2.  0.  0.  1.  3. 60.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  3.  3.  3.  1.  3.  0.  3.  0.  0.  0.  0.  0.  3.  0. 36.]\n",
      "[ 3.  3.  3.  0.  1.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  1.  1.  2.\n",
      "  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 21.]\n",
      "[ 2.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 34.]\n",
      "[ 1.  1.  1.  1.  1.  0.  0.  1.  0.  0.  0.  2.  0.  0.  0.  3.  1.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  0.  2.  0.  0.  2.  3. 52.]\n",
      "[ 3.  2.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  2.  1.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 25.]\n",
      "[ 2.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  1.  2.  0.\n",
      "  3.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.  0. 33.]\n",
      "[ 2.  1.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  1.  2.  0.  2.  2.  2.\n",
      "  2.  2.  2.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  3.  0. 62.]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  1.  0.  0.  0.  3.  1.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  3.  3.  0.  0.  3.  3. 52.]\n",
      "[ 3.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  2.  1.\n",
      "  1.  1.  2.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0. 40.]\n",
      "[ 3.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  2.  2.  0.\n",
      "  3.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 31.]\n",
      "[ 2.  1.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  3.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  3.  0. 27.]\n",
      "[ 3.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  1.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 10.]\n",
      "[ 2.  2.  2.  3.  3.  0.  0.  0.  0.  2.  0.  0.  1.  2.  0.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0. 30.]\n",
      "[ 3.  3.  2.  2.  1.  0.  0.  0.  0.  1.  0.  0.  2.  2.  0.  1.  2.  1.\n",
      "  1.  1.  2.  1.  2.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0. 42.]\n",
      "[ 2.  1.  2.  1.  1.  0.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  3.  0.  3.  0.  3.  0.  0.  3.  3. 48.]\n",
      "[ 0.  1.  2.  1.  1.  0.  1.  0.  2.  3.  0.  0.  0.  1.  0.  0.  3.  1.\n",
      "  2.  3.  3.  3.  1.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0. 22.]\n",
      "[ 3.  2.  1.  3.  0.  0.  0.  0.  0.  0.  1.  0.  1.  2.  0.  3.  2.  0.\n",
      "  1.  0.  1.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  1.  0. 50.]\n",
      "[ 2.  1.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  2.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 43.]\n",
      "[ 3.  3.  3.  3.  3.  0.  0.  0.  3.  3.  1.  0.  0.  1.  0.  0.  2.  1.\n",
      "  2.  2.  2.  2.  2.  2.  0.  1.  0.  0.  0.  0.  0.  2.  0. 42.]\n",
      "[ 2.  1.  1.  3.  2.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  0.  2.  0.  0.  3.  3. 22.]\n",
      "[ 2.  2.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 35.]\n",
      "[ 2.  2.  2.  2.  2.  0.  0.  0.  2.  1.  0.  0.  0.  0.  0.  2.  0.  2.\n",
      "  2.  2.  2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 28.]\n",
      "[ 2.  2.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  2.  2.  1.\n",
      "  1.  0.  1.  0.  1.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 20.]\n",
      "[ 3.  3.  3.  2.  2.  0.  0.  0.  2.  2.  0.  0.  0.  1.  0.  0.  1.  0.\n",
      "  1.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.  0.  0.  3.  0. 43.]\n",
      "[ 2.  3.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  3.  2.  0.\n",
      "  2.  0.  2.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 20.]\n",
      "[ 1.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  2.  3.  0.\n",
      "  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.  0. 34.]\n",
      "[ 2.  2.  2.  2.  2.  0.  0.  0.  2.  2.  1.  0.  0.  1.  0.  0.  2.  1.\n",
      "  2.  2.  2.  1.  0.  2.  0.  0.  1.  0.  0.  0.  0.  1.  0. 39.]\n",
      "[ 1.  1.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  2.  0.  0.  1.  2.\n",
      "  1.  1.  1.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0.  2.  0. 38.]\n",
      "[ 2.  2.  3.  2.  1.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  2.  0.  3.  0.  2.  0.  0.  2.  3. 44.]\n",
      "[ 3.  2.  1.  2.  2.  0.  0.  0.  2.  2.  0.  0.  0.  1.  0.  0.  2.  3.\n",
      "  2.  2.  2.  3.  0.  3.  0.  0.  0.  0.  0.  0.  0.  3.  1. 36.]\n",
      "[ 2.  2.  2.  3.  2.  0.  0.  0.  3.  3.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  2.  2.  2.  2.  2.  0.  1.  0.  0.  0.  0.  0.  2.  0. 41.]\n",
      "[ 2.  2.  2.  3.  2.  2.  0.  2.  0.  0.  0.  3.  0.  0.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  2.  3.  0.  0.  3.  3. 18.]\n",
      "[ 2.  2.  3.  2.  3.  3.  0.  3.  0.  0.  0.  2.  0.  0.  0.  2.  3.  0.\n",
      "  2.  0.  0.  0.  0.  0.  3.  0.  2.  2.  2.  0.  0.  2.  2. 40.]\n",
      "[ 1.  1.  1.  2.  2.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  0.  2.  0.  0.  3.  3. 47.]\n",
      "[ 2.  2.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 27.]\n",
      "[ 2.  2.  3.  2.  2.  2.  0.  3.  0.  0.  0.  2.  0.  0.  2.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  3.  2.  0.  0.  2.  2. 52.]\n",
      "[1. 1. 1. 1. 1. 0. 0. 0. 2. 2. 0. 0. 0. 2. 0. 0. 2. 1. 1. 2. 2. 1. 0. 2.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 2. 0.]\n",
      "[ 3.  3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "  2.  0.  1.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 33.]\n",
      "[ 2.  3.  3.  3.  3.  0.  0.  0.  2.  1.  0.  0.  0.  0.  0.  2.  2.  1.\n",
      "  3.  3.  3.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 30.]\n",
      "[ 3.  2.  2.  2.  2.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0.  0.  3.  1.\n",
      "  3.  3.  3.  2.  0.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0. 29.]\n",
      "[ 2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  3.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 23.]\n",
      "[ 2.  2.  2.  2.  3.  2.  0.  3.  0.  0.  0.  3.  0.  0.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  0.  3.  0.  0.  3.  3. 44.]\n",
      "[ 1.  1.  2.  2.  2.  0.  2.  0.  1.  2.  0.  0.  0.  1.  0.  0.  2.  1.\n",
      "  2.  3.  3.  3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0. 55.]\n",
      "[ 2.  2.  2.  3.  2.  0.  0.  0.  1.  1.  0.  0.  0.  1.  0.  0.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.  0.  0.  3.  0. 40.]\n",
      "[ 3.  2.  2.  1.  3.  3.  0.  3.  0.  0.  0.  3.  0.  0.  0.  3.  2.  0.\n",
      "  3.  0.  0.  0.  0.  0.  2.  0.  3.  3.  3.  0.  0.  2.  2. 34.]\n",
      "[ 1.  1.  1.  1.  1.  0.  1.  0.  2.  3.  0.  0.  0.  1.  0.  0.  2.  2.\n",
      "  1.  2.  2.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  3.  0. 25.]\n",
      "[ 3.  3.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  2.  3.  0.\n",
      "  2.  0.  2.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 37.]\n",
      "[ 2.  3.  2.  3.  3.  3.  0.  2.  0.  0.  0.  3.  0.  0.  0.  3.  2.  0.\n",
      "  3.  0.  0.  0.  0.  0.  3.  0.  2.  2.  2.  0.  0.  2.  2. 41.]\n",
      "[ 3.  3.  3.  3.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  2.  1.\n",
      "  1.  1.  2.  1.  1.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0. 32.]\n",
      "[ 2.  2.  1.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 20.]\n",
      "[ 2.  3.  1.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  1.  0.\n",
      "  0.  2.  1.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 19.]\n",
      "[ 3.  2.  2.  3.  2.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0.  2.  2.  1.\n",
      "  2.  3.  3.  3.  3.  1.  0.  0.  0.  0.  0.  0.  0.  3.  0. 61.]\n",
      "[ 2.  1.  1.  3.  2.  2.  0.  2.  0.  0.  0.  0.  0.  0.  2.  3.  0.  2.\n",
      "  0.  0.  0.  0.  0.  3.  0.  2.  0.  2.  2.  0.  0.  3.  2. 27.]\n",
      "[ 1.  1.  2.  3.  2.  3.  0.  3.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  3.  2.  3.  0.  0.  2.  3. 36.]\n",
      "[ 2.  2.  3.  3.  1.  2.  0.  2.  0.  0.  0.  1.  0.  0.  0.  3.  3.  0.\n",
      "  2.  0.  0.  0.  0.  0.  3.  0.  2.  1.  2.  0.  0.  2.  3. 40.]\n",
      "[ 3.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 52.]\n",
      "[ 2.  3.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 45.]\n",
      "[ 1.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 34.]\n",
      "[ 2.  3.  2.  1.  0.  0.  0.  0.  1.  2.  1.  0.  0.  1.  0.  2.  1.  0.\n",
      "  1.  2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 27.]\n",
      "[ 3.  3.  2.  2.  0.  0.  0.  0.  2.  0.  0.  0.  0.  1.  0.  2.  1.  0.\n",
      "  2.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 46.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  2.  0.  1.  3.  0.\n",
      "  1.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 52.]\n",
      "[ 2.  1.  1.  1.  0.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  1.  0.  2.  2.  2.  0.  0.  3.  3. 40.]\n",
      "[ 2.  3.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 55.]\n",
      "[ 3.  3.  2.  2.  0.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  2.  3.  3.  0.  0.  2.  3. 32.]\n",
      "[ 2.  2.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 33.]\n",
      "[ 3.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 47.]\n",
      "[ 2.  1.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  3.  0. 61.]\n",
      "[ 3.  2.  2.  2.  0.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  1.  1.  1.  0.  0.  2.  2. 22.]\n",
      "[ 3.  2.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0. 20.]\n",
      "[ 3.  2.  2.  3.  1.  0.  0.  0.  1.  1.  1.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  2.  3.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 55.]\n",
      "[ 2.  3.  2.  2.  1.  0.  0.  0.  2.  2.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  3.  3.  3.  2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0. 67.]\n",
      "[ 2.  2.  3.  3.  1.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  3.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  2.  0.  0.  0.  0.  2.  3. 51.]\n",
      "[ 2.  2.  3.  3.  1.  3.  0.  2.  0.  0.  0.  3.  0.  0.  0.  2.  3.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  3.  2.  2.  0.  0.  3.  3. 22.]\n",
      "[ 1.  1.  2.  3.  1.  2.  0.  2.  0.  0.  0.  1.  0.  0.  0.  1.  3.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  2.  2.  3.  0.  0.  2.  3. 45.]\n",
      "[ 2.  3.  2.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  3.  2.  1.  3.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 55.]\n",
      "[ 3.  2.  3.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.  3.  0.  0.  1.  0.\n",
      "  1.  2.  2.  0.  2.  0.  0.  3.  0.  0.  0.  0.  0.  1.  0. 56.]\n",
      "[ 3.  3.  3.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  2.  3.  0.  3.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 18.]\n",
      "[ 3.  2.  2.  2.  0.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  3.  0.\n",
      "  3.  0.  0.  0.  0.  0.  2.  0.  2.  3.  2.  0.  0.  2.  3. 40.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  3.  2.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  2.  3.  2.  3.  0.  0.  2.  0.  0.  0.  0.  0.  3.  0. 30.]\n",
      "[ 3.  2.  3.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  2.  0.  0.  3.  0.\n",
      "  2.  2.  2.  3.  3.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 33.]\n",
      "[ 2.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  3.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 40.]\n",
      "[ 1.  2.  2.  2.  0.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  3.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  3.  0.  2.  2.  2.  0.  0.  2.  2. 42.]\n",
      "[ 2.  2.  2.  2.  0.  2.  0.  3.  0.  0.  0.  3.  0.  0.  0.  3.  3.  0.\n",
      "  3.  0.  0.  0.  0.  0.  3.  0.  3.  3.  3.  0.  0.  2.  2. 36.]\n",
      "[ 2.  3.  2.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  3.  0.  0.  3.  0.\n",
      "  2.  2.  2.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 27.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  1.  0.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 56.]\n",
      "[ 1.  3.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  0.  3.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0. 60.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  3.  0.\n",
      "  3.  0.  3.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 20.]\n",
      "[ 2.  2.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  1.  0. 30.]\n",
      "[ 3.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  2.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 19.]\n",
      "[ 2.  3.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  3.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 52.]\n",
      "[ 3.  2.  0.  2.  0.  0.  0.  0.  0.  0.  1.  0.  0.  2.  0.  2.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  3.  0. 55.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  2.  2.  0.  2.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 23.]\n",
      "[ 2.  1.  2.  3.  1.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  1.  3.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  2.  0.  3.  0.  0.  3.  2. 50.]\n",
      "[ 2.  3.  3.  3.  0.  3.  0.  3.  0.  0.  0.  2.  0.  0.  0.  2.  1.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  2.  0.  3.  0.  0.  2.  2. 38.]\n",
      "[ 3.  2.  2.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0.  0.  0.  3.  2.  0.\n",
      "  3.  0.  0.  0.  0.  0.  2.  0.  3.  0.  2.  0.  0.  3.  3. 25.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  2.  2.  1.  0.  0.  2.  0.  0.  2.  0.\n",
      "  3.  3.  2.  2.  2.  1.  0.  2.  0.  0.  0.  0.  0.  2.  0. 18.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  3.  3.  1.  0.  0.  3.  0.  0.  3.  0.\n",
      "  2.  3.  2.  3.  2.  0.  0.  3.  0.  0.  0.  0.  0.  2.  0. 35.]\n",
      "[ 2.  2.  1.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  1.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 22.]\n",
      "[ 3.  2.  3.  3.  1.  2.  0.  2.  0.  0.  0.  3.  0.  0.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  1.  0.  2.  0.  2.  0.  0.  2.  3. 52.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  1.  1.  0.\n",
      "  2.  0.  0.  0.  1.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 50.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  2.  0.  2.  0.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 33.]\n",
      "[ 3.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  2.  0.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 44.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  2.  2.  1.  0.  0.  2.  0.  0.  3.  2.\n",
      "  1.  2.  2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0. 55.]\n",
      "[ 2.  3.  2.  0.  1.  0.  0.  0.  2.  3.  0.  0.  0.  2.  0.  0.  2.  3.\n",
      "  1.  2.  2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0. 20.]\n",
      "[ 3.  2.  2.  0.  2.  0.  0.  0.  3.  2.  0.  0.  0.  3.  0.  0.  3.  2.\n",
      "  2.  2.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 60.]\n",
      "[ 2.  3.  2.  0.  0.  0.  0.  0.  2.  3.  0.  0.  0.  2.  0.  0.  2.  2.\n",
      "  2.  2.  2.  3.  2.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0. 33.]\n",
      "[ 2.  3.  2.  0.  0.  0.  0.  0.  3.  2.  0.  0.  0.  2.  0.  0.  3.  2.\n",
      "  3.  2.  2.  3.  2.  1.  0.  0.  0.  0.  0.  0.  0.  2.  0. 27.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  2.  0.  0.  3.  1.\n",
      "  2.  3.  3.  2.  3.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0. 50.]\n",
      "[ 2.  3.  2.  0.  0.  0.  0.  0.  3.  2.  0.  0.  0.  2.  0.  0.  2.  1.\n",
      "  2.  3.  3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. 70.]\n",
      "[ 2.  2.  2.  3.  1.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  2.  2.  1.  3.  0.  0.  2.  3. 28.]\n",
      "[ 2.  2.  2.  2.  1.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  3.  2.  2.  2.  2.  0.  0.  3.  3. 30.]\n",
      "[ 3.  2.  3.  2.  2.  2.  0.  2.  0.  0.  0.  3.  0.  0.  0.  2.  3.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  1.  2.  2.  1.  0.  0.  2.  3. 53.]\n",
      "[ 2.  3.  2.  3.  3.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  3.  2.  2.  1.  2.  0.  0.  2.  2. 27.]\n",
      "[ 2.  2.  3.  2.  2.  2.  0.  3.  0.  0.  0.  3.  0.  0.  0.  2.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  2.  2.  3.  2.  2.  0.  0.  2.  3. 50.]\n",
      "[ 3.  2.  2.  3.  3.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  2.  2.  2.  2.  2.  0.  0.  2.  2. 42.]\n",
      "[ 2.  2.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 47.]\n",
      "[ 3.  2.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  2.  2.  0.  3.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  3.  0. 30.]\n",
      "[ 2.  3.  0.  3.  0.  0.  0.  0.  0.  2.  0.  0.  2.  2.  0.  2.  3.  0.\n",
      "  3.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 42.]\n",
      "[ 3.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0.  0.  2.  2.  0.  2.  2.  0.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  2.  0. 55.]\n",
      "[ 2.  2.  2.  1.  1.  0.  0.  0.  2.  0.  1.  0.  0.  2.  0.  1.  2.  1.\n",
      "  2.  2.  2.  2.  1.  1.  0.  1.  0.  0.  0.  0.  0.  2.  0. 60.]\n",
      "[ 3.  2.  3.  0.  1.  0.  0.  0.  1.  2.  0.  0.  0.  2.  0.  2.  1.  2.\n",
      "  1.  1.  1.  1.  1.  1.  0.  2.  0.  0.  0.  0.  0.  2.  0. 65.]\n",
      "[ 2.  2.  2.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  1.  0.  2.  1.  2.\n",
      "  2.  2.  1.  2.  0.  0.  0.  1.  0.  0.  0.  0.  0.  2.  0. 47.]\n",
      "[ 3.  2.  3.  2.  0.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.  2.  1.  3.\n",
      "  2.  2.  2.  2.  0.  0.  0.  3.  0.  0.  0.  0.  0.  3.  0. 35.]\n",
      "[ 2.  2.  1.  1.  0.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.  2.  2.  2.\n",
      "  2.  1.  2.  2.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 52.]\n",
      "[ 2.  2.  2.  2.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  2.  2.  3.\n",
      "  2.  2.  1.  1.  0.  0.  0.  2.  0.  0.  0.  0.  0.  1.  0. 60.]\n",
      "[ 2.  2.  2.  3.  0.  0.  0.  0.  2.  2.  1.  0.  0.  1.  0.  0.  2.  1.\n",
      "  2.  3.  3.  3.  0.  1.  0.  2.  0.  0.  0.  0.  0.  1.  0. 25.]\n",
      "[ 3.  2.  2.  3.  0.  0.  0.  0.  2.  0.  0.  0.  0.  2.  0.  0.  2.  2.\n",
      "  2.  2.  2.  2.  0.  2.  0.  2.  0.  0.  0.  0.  0.  1.  0. 60.]\n",
      "[ 2.  2.  2.  2.  0.  0.  0.  0.  2.  2.  1.  0.  0.  1.  0.  0.  3.  0.\n",
      "  3.  2.  2.  2.  0.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 50.]\n",
      "[ 2.  1.  2.  0.  0.  0.  0.  0.  3.  2.  1.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  3.  2.  2.  0.  2.  0.  3.  0.  0.  0.  0.  0.  2.  0. 33.]\n",
      "[ 3.  3.  3.  0.  0.  0.  0.  0.  2.  3.  0.  0.  0.  1.  0.  0.  3.  0.\n",
      "  3.  3.  3.  3.  0.  1.  0.  3.  0.  0.  0.  0.  0.  3.  0. 27.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  2.  3.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  3.  2.  2.  0.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 55.]\n",
      "[ 2.  2.  2.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  2.  0.  0.  2.  0.\n",
      "  2.  2.  2.  2.  0.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 62.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  2.  0.  0.  2.  0.\n",
      "  1.  1.  2.  2.  1.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 19.]\n",
      "[ 2.  1.  1.  2.  2.  2.  0.  2.  0.  0.  0.  2.  1.  0.  0.  3.  2.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  2.  3.  0.  0.  2.  3. 50.]\n",
      "[ 2.  2.  2.  2.  3.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  3.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  3.  2.  0.  0.  3.  2. 40.]\n",
      "[ 2.  2.  2.  2.  1.  2.  0.  2.  0.  0.  0.  2.  1.  0.  0.  3.  2.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  2.  3.  0.  0.  2.  3. 62.]\n",
      "[ 2.  2.  2.  2.  1.  2.  0.  2.  0.  0.  0.  3.  1.  0.  0.  2.  2.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  3.  2.  0.  0.  2.  3. 36.]\n",
      "[ 2.  1.  2.  2.  0.  3.  0.  2.  0.  0.  0.  2.  1.  0.  0.  3.  2.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  3.  2.  0.  0.  3.  2. 27.]\n",
      "[ 3.  1.  2.  3.  0.  3.  0.  0.  0.  1.  0.  2.  1.  0.  0.  2.  3.  1.\n",
      "  2.  0.  0.  0.  0.  0.  1.  0.  3.  2.  3.  0.  0.  2.  2. 47.]\n",
      "[ 2.  1.  2.  2.  0.  2.  0.  0.  0.  1.  0.  1.  1.  0.  0.  2.  3.  1.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  2.  2.  2.  0.  0.  2.  2. 50.]\n",
      "[ 3.  3.  2.  0.  1.  0.  0.  0.  2.  2.  1.  0.  0.  0.  0.  0.  2.  1.\n",
      "  3.  3.  3.  2.  1.  0.  0.  1.  0.  0.  0.  0.  0.  2.  0. 30.]\n",
      "[ 2.  2.  3.  1.  0.  0.  0.  0.  3.  2.  1.  0.  0.  0.  0.  1.  2.  0.\n",
      "  3.  3.  3.  2.  1.  1.  0.  2.  0.  0.  0.  0.  0.  3.  0. 57.]\n",
      "[ 1.  2.  2.  1.  1.  0.  0.  0.  2.  2.  1.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  3.  3.  3.  2.  1.  0.  2.  0.  0.  0.  0.  0.  2.  0. 62.]\n",
      "[ 2.  2.  3.  1.  0.  0.  0.  0.  1.  2.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  3.  3.  2.  1.  1.  0.  2.  0.  0.  0.  0.  0.  2.  0. 36.]\n",
      "[ 3.  2.  2.  2.  0.  0.  0.  0.  2.  1.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  3.  2.  3.  2.  2.  1.  0.  2.  0.  0.  0.  0.  0.  3.  0. 18.]\n",
      "[ 2.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  2.  1.\n",
      "  2.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  2.  0. 25.]\n",
      "[ 3.  2.  2.  1.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  3.  0. 16.]\n",
      "[ 3.  3.  2.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  0.  0.  0.  2.  1.\n",
      "  2.  2.  3.  2.  1.  0.  0.  1.  0.  0.  0.  0.  0.  2.  0. 55.]\n",
      "[ 2.  3.  3.  1.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.  0.  0.  3.  0.\n",
      "  2.  2.  2.  3.  0.  1.  0.  2.  0.  0.  0.  0.  0.  2.  0. 22.]\n",
      "[ 2.  2.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  2.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0. 70.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  2.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  3.  0. 22.]\n",
      "[ 2.  3.  2.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  3.  0. 45.]\n",
      "[ 2.  1.  2.  1.  0.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  0.  2.  0.  0.  3.  3. 40.]\n",
      "[ 2.  2.  3.  1.  0.  3.  0.  2.  0.  0.  0.  3.  0.  0.  0.  3.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  0.  3.  2.  3.  0.  0.  3.  3. 28.]\n",
      "[ 3.  2.  2.  1.  0.  2.  0.  3.  0.  0.  0.  2.  0.  0.  0.  2.  2.  1.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  1.  2.  0.  0.  3.  3. 36.]\n",
      "[ 2.  2.  3.  2.  1.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  2.  0.  2.  2.  2.  0.  0.  3.  3. 27.]\n",
      "[ 2.  2.  2.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  2.  1.\n",
      "  2.  3.  3.  2.  0.  2.  0.  1.  0.  0.  0.  0.  0.  2.  0. 42.]\n",
      "[ 2.  3.  2.  0.  1.  0.  0.  0.  3.  2.  1.  0.  0.  1.  0.  0.  3.  0.\n",
      "  2.  2.  3.  3.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 27.]\n",
      "[ 2.  2.  3.  1.  0.  0.  0.  0.  2.  2.  0.  0.  0.  0.  0.  0.  3.  1.\n",
      "  3.  3.  2.  2.  2.  1.  0.  3.  0.  0.  0.  0.  0.  2.  0. 50.]\n",
      "[ 2.  1.  3.  0.  1.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  3.  0.\n",
      "  2.  2.  3.  3.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 34.]\n",
      "[ 2.  2.  3.  3.  1.  2.  0.  1.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  2.  0.  2.  0.  0.  2.  3. 25.]\n",
      "[ 3.  2.  2.  2.  0.  2.  0.  2.  0.  0.  0.  3.  0.  0.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  2.  0.  3.  0.  0.  3.  3. 36.]\n",
      "[ 2.  2.  2.  3.  1.  2.  0.  1.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  1.  0.  2.  0.  3.  0.  0.  1.  2. 50.]\n",
      "[ 3.  1.  2.  1.  0.  0.  0.  0.  2.  3.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "  2.  3.  2.  2.  0.  3.  0.  2.  0.  0.  0.  0.  0.  2.  0. 17.]\n",
      "[ 2.  2.  2.  0.  1.  0.  0.  0.  2.  1.  0.  0.  0.  0.  0.  0.  3.  1.\n",
      "  1.  3.  2.  2.  0.  2.  0.  0.  0.  0.  0.  0.  0.  2.  0. 24.]\n",
      "[ 2.  3.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "  2.  3.  3.  2.  0.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 43.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  3.  0.\n",
      "  2.  2.  3.  2.  0.  1.  0.  2.  0.  0.  0.  0.  0.  2.  0. 50.]\n",
      "[ 2.  2.  2.  1.  0.  0.  0.  0.  2.  2.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "  2.  2.  3.  2.  0.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 36.]\n",
      "[ 2.  2.  3.  3.  2.  3.  0.  1.  0.  0.  0.  2.  0.  0.  0.  3.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  0.  3.  0.  0.  2.  3. 26.]\n",
      "[ 3.  1.  2.  3.  2.  2.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  0.  2.  0.  0.  3.  3. 16.]\n",
      "[ 2.  2.  2.  3.  2.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  3.  0.  2.  0.  2.  0.  0.  2.  3. 32.]\n",
      "[ 2.  1.  2.  3.  3.  2.  0.  2.  0.  0.  0.  3.  0.  0.  0.  3.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  2.  0.  3.  0.  2.  0.  0.  2.  3. 51.]\n",
      "[ 2.  2.  3.  2.  2.  3.  0.  1.  0.  0.  0.  2.  0.  0.  0.  3.  3.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  3.  0.  3.  0.  0.  3.  2. 56.]\n",
      "[ 2.  2.  2.  0.  1.  0.  0.  0.  2.  2.  0.  0.  0.  0.  0.  0.  2.  1.\n",
      "  2.  3.  2.  3.  1.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 47.]\n",
      "[ 2.  2.  3.  1.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.  0.  0.  2.  2.\n",
      "  3.  3.  3.  2.  0.  1.  0.  2.  0.  0.  0.  0.  0.  2.  0. 51.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  0.  0.  0.  3.  0.\n",
      "  3.  2.  3.  3.  0.  2.  0.  3.  0.  0.  0.  0.  0.  3.  0. 58.]\n",
      "[ 2.  2.  3.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.  0.  0.  0.  3.  2.\n",
      "  2.  2.  3.  2.  0.  0.  0.  1.  0.  0.  0.  0.  0.  2.  0. 27.]\n",
      "[ 2.  3.  3.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  0.  0.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  0.  1.  0.  0.  0.  0.  0.  2.  0. 62.]\n",
      "[ 2.  2.  3.  0.  1.  0.  0.  0.  3.  0.  0.  0.  0.  2.  0.  0.  1.  1.\n",
      "  2.  2.  2.  3.  0.  2.  0.  1.  0.  0.  0.  0.  0.  2.  0. 53.]\n",
      "[ 2.  1.  2.  3.  2.  2.  0.  3.  0.  0.  0.  2.  1.  0.  0.  2.  2.  0.\n",
      "  1.  0.  0.  0.  0.  0.  2.  0.  2.  1.  2.  0.  0.  2.  3. 37.]\n",
      "[ 1.  1.  2.  3.  2.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  2.  1.  0.\n",
      "  2.  0.  0.  0.  0.  0.  2.  0.  2.  0.  2.  0.  0.  2.  3. 49.]\n",
      "[ 2.  2.  2.  2.  2.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "  2.  2.  3.  3.  2.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 46.]\n",
      "[ 2.  1.  2.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  1.  0.  0.  2.  1.\n",
      "  2.  3.  2.  3.  0.  0.  0.  3.  0.  0.  0.  0.  0.  0.  0. 33.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  0.  0.  0.  3.  0.\n",
      "  3.  2.  3.  2.  0.  1.  0.  2.  0.  0.  0.  0.  0.  1.  0. 56.]\n",
      "[ 2.  3.  2.  1.  0.  0.  0.  0.  2.  2.  0.  0.  0.  0.  0.  0.  2.  0.\n",
      "  2.  2.  3.  2.  1.  0.  0.  2.  0.  0.  0.  0.  0.  2.  0. 60.]\n",
      "[ 2.  1.  2.  2.  2.  3.  0.  2.  0.  0.  0.  2.  1.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  3.  0.  2.  0.  2.  0.  0.  2.  3. 51.]\n",
      "[ 2.  2.  3.  0.  0.  0.  0.  0.  2.  2.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  3.  2.  3.  0.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 33.]\n",
      "[ 1.  2.  2.  2.  0.  0.  0.  0.  2.  2.  0.  0.  0.  1.  0.  0.  2.  1.\n",
      "  3.  3.  3.  2.  0.  2.  0.  2.  0.  0.  0.  0.  0.  2.  0. 25.]\n",
      "[2. 2. 2. 3. 2. 0. 0. 0. 2. 3. 1. 0. 0. 1. 0. 0. 2. 2. 2. 2. 2. 2. 0. 2.\n",
      " 0. 3. 0. 0. 0. 0. 0. 2. 0. 9.]\n",
      "[ 3.  2.  2.  3.  2.  0.  0.  0.  2.  3.  0.  0.  0.  0.  0.  0.  3.  0.\n",
      "  2.  2.  3.  2.  0.  3.  0.  2.  0.  0.  0.  0.  0.  1.  0. 55.]\n",
      "[ 2.  3.  2.  3.  2.  0.  0.  0.  3.  2.  0.  0.  0.  1.  0.  0.  3.  2.\n",
      "  3.  2.  2.  2.  0.  3.  0.  3.  0.  0.  0.  0.  0.  0.  0. 36.]\n",
      "[ 2.  2.  2.  2.  2.  0.  0.  0.  3.  0.  1.  0.  0.  0.  0.  0.  2.  2.\n",
      "  2.  2.  3.  3.  0.  2.  0.  3.  0.  0.  0.  0.  0.  0.  0. 75.]\n",
      "[ 2.  2.  2.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  3.  2.  3.  2.  1.  0.  1.  0.  0.  0.  0.  0.  2.  0. 45.]\n",
      "[ 2.  3.  2.  1.  0.  0.  0.  0.  2.  2.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  2.  2.  2.  0.  2.  0.  2.  0.  0.  0.  0.  0.  3.  0. 24.]\n",
      "[ 2.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  1.  0. 40.]\n",
      "[ 2.  2.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  2.  2.  1.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  1.  0. 25.]\n",
      "[ 3.  2.  2.  2.  3.  2.  0.  2.  0.  0.  0.  2.  2.  0.  0.  3.  3.  0.\n",
      "  0.  0.  0.  0.  0.  0.  3.  0.  3.  0.  3.  0.  0.  2.  3. 28.]\n",
      "[ 2.  1.  3.  1.  2.  3.  0.  2.  0.  0.  0.  2.  0.  0.  0.  3.  2.  0.\n",
      "  0.  0.  0.  0.  0.  0.  3.  0.  2.  0.  1.  0.  0.  2.  3. 50.]\n",
      "[ 3.  2.  2.  0.  0.  0.  0.  0.  3.  3.  0.  0.  0.  1.  0.  0.  2.  0.\n",
      "  2.  3.  2.  3.  0.  2.  0.  2.  0.  0.  0.  0.  0.  3.  0. 35.]\n"
     ]
    }
   ],
   "source": [
    "for r in dataset_in:\n",
    "    print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_dataset, test_dataset, \\\n",
    "train_labels, test_labels = train_test_split(\n",
    "    dataset_in, labels_onehot, test_size = .1, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(dataset):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_dataset, test_dataset, \\\n",
    "    train_labels, test_labels = train_test_split(\n",
    "        dataset_in, labels_onehot, test_size = .2, random_state=12)\n",
    "    network = NeuralNetwork(train_dataset.shape[1],4,train_labels.shape[1])\n",
    "    network.train(train_dataset,train_labels,0.01, 0.05)\n",
    "    network.predict(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 13.2506225075\n",
      "Loss at step 500: 1.5311149643\n",
      "Loss at step 1000: 1.34450680608\n",
      "Loss at step 1500: 1.23144821691\n",
      "Loss at step 2000: 1.59262675034\n",
      "Loss at step 2500: 0.315126922454\n",
      "Loss at step 3000: 0.232023580008\n",
      "Loss at step 3500: 0.205518216\n",
      "Loss at step 4000: 0.17515667508\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "NN(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(loc):\n",
    "    df=pd.read_csv(loc, sep=',',header=None)\n",
    "    df = df[((df[16]==0) | (df[16]==1) | (df[16]==2) | (df[16]==3))]\n",
    "    dataset = df.values.astype('float')\n",
    "    #np.random.shuffle(dataset)\n",
    "    return dataset\n",
    "train = read_dataset('pendigits/pendigits.tra')\n",
    "test = read_dataset('pendigits/pendigits.tes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in = train[:,:-1]\n",
    "train_out = one_hot_encoding(train[:,-1], 4)\n",
    "test_in = test[:,:-1]\n",
    "test_out = one_hot_encoding(test[:,-1],4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN(train_x, train_y, test_x, test_y, hidden_layer):\n",
    "    network = NeuralNetwork(train_x.shape[1],hidden_layer,train_y.shape[1])\n",
    "    network.train(train_x,train_y,0.01, 0.001)\n",
    "    network.predict(test_x, test_y)\n",
    "# learning rate: 0.0005 gives 95% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "Loss at step 0: 27.981513725\n",
      "Loss at step 500: 1.80987458266\n",
      "Loss at step 1000: 1.51911303061\n",
      "Loss at step 1500: 1.78946199782\n",
      "Loss at step 2000: 1.31729176288\n",
      "Loss at step 2500: 1.33095732164\n",
      "92.0112123336\n"
     ]
    }
   ],
   "source": [
    "print test_in.dtype\n",
    "NN(train_in, train_out, test_in, test_out,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, num_labels):\n",
    "    labels_onehot = np.zeros((labels.shape[0], num_labels)).astype(int)\n",
    "    labels_onehot[np.arange(len(labels)), labels.astype(int)] = 1\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_in.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satyam/.local/lib/python2.7/site-packages/ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-36.04365338911715"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(sys.float_info.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 35)"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3058, 16)\n"
     ]
    }
   ],
   "source": [
    "print train_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.  89.  27. ...   2. 100.   6.]\n",
      " [  0.  57.  31. ...  25.  16.   0.]\n",
      " [  0.  67.  49. ...  20.  47.   0.]\n",
      " ...\n",
      " [ 55.  89.  85. ...   0.   0.   3.]\n",
      " [ 17.  63.   6. ...  91.   0.  52.]\n",
      " [ 59.  65.  91. ...   1. 100.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print train_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, LSTM\n",
    "def NeuralNetwork(XTrain, YTrain, XTest, YTest):\n",
    "\tmodel = Sequential()\n",
    "\tprint np.shape(XTrain)\n",
    "\tprint np.shape(YTrain)\n",
    "\tmodel.add(Dense(16, activation='relu', input_dim=np.shape(XTrain)[1]))\n",
    "\t# model.add(Dense(128, activation='relu'))\n",
    "\t# model.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(5, activation='relu'))\n",
    "\tmodel.add(Dropout(0.1))\n",
    "\tmodel.add(Dense(4, activation='softmax'))\n",
    "\tsgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\tmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\tx_train=np.array(XTrain)\n",
    "\tx_test=np.array(XTest)\n",
    "\ty_train=np.array(YTrain)\n",
    "\ty_test=np.array(YTest)\n",
    "\n",
    "\ty_train=y_train[:,np.newaxis]\n",
    "\ty_test=y_test[:,np.newaxis]\n",
    "\tmapping = YTrain\n",
    "\tprint \"len mapping : \",len(mapping)\n",
    "\tprint mapping\n",
    "\n",
    "\n",
    "\ty_train=keras.utils.to_categorical(y_train, num_classes=4)\n",
    "\ty_test=keras.utils.to_categorical(y_test, num_classes=4)\n",
    "\n",
    "\n",
    "\tprint \"len train set\",np.shape(x_train),np.shape(y_train)\n",
    "\tprint \"len test set\",np.shape(x_test),np.shape(y_test)\n",
    "\tmodel.fit(x_train, y_train,epochs=100,batch_size=128,shuffle=True,validation_split=0.25)\n",
    "\tacc= model.evaluate(x_test, y_test, batch_size=128)\n",
    "\tprint acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3058, 16)\n",
      "(3058,)\n",
      "len mapping :  3058\n",
      "[2. 1. 1. ... 3. 0. 1.]\n",
      "len train set (3058, 16) (3058, 4)\n",
      "len test set (1427, 16) (1427, 4)\n",
      "Train on 2293 samples, validate on 765 samples\n",
      "Epoch 1/100\n",
      "2293/2293 [==============================] - 0s - loss: 2.7197 - acc: 0.5678 - val_loss: 0.6316 - val_acc: 0.7843\n",
      "Epoch 2/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.6049 - acc: 0.7758 - val_loss: 0.3866 - val_acc: 0.8562\n",
      "Epoch 3/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.4799 - acc: 0.7985 - val_loss: 0.3131 - val_acc: 0.8797\n",
      "Epoch 4/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.4302 - acc: 0.8072 - val_loss: 0.2909 - val_acc: 0.9007\n",
      "Epoch 5/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.4132 - acc: 0.8155 - val_loss: 0.2621 - val_acc: 0.9098\n",
      "Epoch 6/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3895 - acc: 0.8373 - val_loss: 0.2480 - val_acc: 0.9137\n",
      "Epoch 7/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3576 - acc: 0.8578 - val_loss: 0.2344 - val_acc: 0.9216\n",
      "Epoch 8/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3362 - acc: 0.8622 - val_loss: 0.2188 - val_acc: 0.9294\n",
      "Epoch 9/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3280 - acc: 0.8565 - val_loss: 0.2052 - val_acc: 0.9346\n",
      "Epoch 10/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3322 - acc: 0.8543 - val_loss: 0.1970 - val_acc: 0.9359\n",
      "Epoch 11/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3221 - acc: 0.8552 - val_loss: 0.1897 - val_acc: 0.9333\n",
      "Epoch 12/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3121 - acc: 0.8556 - val_loss: 0.1733 - val_acc: 0.9359\n",
      "Epoch 13/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.3112 - acc: 0.8526 - val_loss: 0.1742 - val_acc: 0.9412\n",
      "Epoch 14/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2973 - acc: 0.8870 - val_loss: 0.1710 - val_acc: 0.9399\n",
      "Epoch 15/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2850 - acc: 0.8879 - val_loss: 0.1530 - val_acc: 0.9399\n",
      "Epoch 16/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2860 - acc: 0.8870 - val_loss: 0.1435 - val_acc: 0.9451\n",
      "Epoch 17/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2688 - acc: 0.8949 - val_loss: 0.1365 - val_acc: 0.9503\n",
      "Epoch 18/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2657 - acc: 0.8962 - val_loss: 0.1293 - val_acc: 0.9529\n",
      "Epoch 19/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2491 - acc: 0.9093 - val_loss: 0.1189 - val_acc: 0.9516\n",
      "Epoch 20/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2583 - acc: 0.8988 - val_loss: 0.1198 - val_acc: 0.9556\n",
      "Epoch 21/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2381 - acc: 0.9128 - val_loss: 0.1146 - val_acc: 0.9569\n",
      "Epoch 22/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2498 - acc: 0.9023 - val_loss: 0.1099 - val_acc: 0.9582\n",
      "Epoch 23/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2387 - acc: 0.9115 - val_loss: 0.1133 - val_acc: 0.9569\n",
      "Epoch 24/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2286 - acc: 0.9132 - val_loss: 0.1067 - val_acc: 0.9595\n",
      "Epoch 25/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2229 - acc: 0.9180 - val_loss: 0.1034 - val_acc: 0.9595\n",
      "Epoch 26/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2342 - acc: 0.9080 - val_loss: 0.1014 - val_acc: 0.9608\n",
      "Epoch 27/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2333 - acc: 0.9198 - val_loss: 0.1037 - val_acc: 0.9660\n",
      "Epoch 28/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2199 - acc: 0.9202 - val_loss: 0.0993 - val_acc: 0.9621\n",
      "Epoch 29/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2298 - acc: 0.9132 - val_loss: 0.0965 - val_acc: 0.9647\n",
      "Epoch 30/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2206 - acc: 0.9102 - val_loss: 0.0985 - val_acc: 0.9647\n",
      "Epoch 31/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2392 - acc: 0.9102 - val_loss: 0.0957 - val_acc: 0.9647\n",
      "Epoch 32/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2147 - acc: 0.9228 - val_loss: 0.0960 - val_acc: 0.9699\n",
      "Epoch 33/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2172 - acc: 0.9171 - val_loss: 0.0958 - val_acc: 0.9686\n",
      "Epoch 34/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2120 - acc: 0.9198 - val_loss: 0.0939 - val_acc: 0.9686\n",
      "Epoch 35/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2166 - acc: 0.9158 - val_loss: 0.0897 - val_acc: 0.9699\n",
      "Epoch 36/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2045 - acc: 0.9184 - val_loss: 0.0890 - val_acc: 0.9712\n",
      "Epoch 37/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2161 - acc: 0.9145 - val_loss: 0.0894 - val_acc: 0.9699\n",
      "Epoch 38/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2057 - acc: 0.9184 - val_loss: 0.0894 - val_acc: 0.9686\n",
      "Epoch 39/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2003 - acc: 0.9206 - val_loss: 0.0811 - val_acc: 0.9725\n",
      "Epoch 40/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2012 - acc: 0.9224 - val_loss: 0.0815 - val_acc: 0.9725\n",
      "Epoch 41/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2011 - acc: 0.9254 - val_loss: 0.0817 - val_acc: 0.9712\n",
      "Epoch 42/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.2020 - acc: 0.9189 - val_loss: 0.0881 - val_acc: 0.9725\n",
      "Epoch 43/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1987 - acc: 0.9276 - val_loss: 0.0781 - val_acc: 0.9725\n",
      "Epoch 44/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1886 - acc: 0.9259 - val_loss: 0.0793 - val_acc: 0.9765\n",
      "Epoch 45/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1979 - acc: 0.9254 - val_loss: 0.0749 - val_acc: 0.9752\n",
      "Epoch 46/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1930 - acc: 0.9272 - val_loss: 0.0758 - val_acc: 0.9739\n",
      "Epoch 47/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1869 - acc: 0.9337 - val_loss: 0.0756 - val_acc: 0.9739\n",
      "Epoch 48/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1910 - acc: 0.9285 - val_loss: 0.0737 - val_acc: 0.9765\n",
      "Epoch 49/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1948 - acc: 0.9232 - val_loss: 0.0746 - val_acc: 0.9752\n",
      "Epoch 50/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1831 - acc: 0.9320 - val_loss: 0.0696 - val_acc: 0.9765\n",
      "Epoch 51/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1781 - acc: 0.9320 - val_loss: 0.0705 - val_acc: 0.9765\n",
      "Epoch 52/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1864 - acc: 0.9232 - val_loss: 0.0686 - val_acc: 0.9765\n",
      "Epoch 53/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1778 - acc: 0.9355 - val_loss: 0.0684 - val_acc: 0.9778\n",
      "Epoch 54/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1893 - acc: 0.9298 - val_loss: 0.0712 - val_acc: 0.9778\n",
      "Epoch 55/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1789 - acc: 0.9368 - val_loss: 0.0662 - val_acc: 0.9752\n",
      "Epoch 56/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1801 - acc: 0.9320 - val_loss: 0.0663 - val_acc: 0.9765\n",
      "Epoch 57/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1728 - acc: 0.9324 - val_loss: 0.0626 - val_acc: 0.9765\n",
      "Epoch 58/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1769 - acc: 0.9328 - val_loss: 0.0627 - val_acc: 0.9765\n",
      "Epoch 59/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1673 - acc: 0.9363 - val_loss: 0.0612 - val_acc: 0.9765\n",
      "Epoch 60/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1734 - acc: 0.9315 - val_loss: 0.0617 - val_acc: 0.9778\n",
      "Epoch 61/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1764 - acc: 0.9280 - val_loss: 0.0622 - val_acc: 0.9791\n",
      "Epoch 62/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1811 - acc: 0.9341 - val_loss: 0.0617 - val_acc: 0.9804\n",
      "Epoch 63/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1634 - acc: 0.9385 - val_loss: 0.0575 - val_acc: 0.9778\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2293/2293 [==============================] - 0s - loss: 0.1736 - acc: 0.9333 - val_loss: 0.0592 - val_acc: 0.9804\n",
      "Epoch 65/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1721 - acc: 0.9429 - val_loss: 0.0566 - val_acc: 0.9791\n",
      "Epoch 66/100\n",
      "2293/2293 [==============================] - ETA: 0s - loss: 0.1735 - acc: 0.921 - 0s - loss: 0.1662 - acc: 0.9376 - val_loss: 0.0577 - val_acc: 0.9804\n",
      "Epoch 67/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1577 - acc: 0.9416 - val_loss: 0.0557 - val_acc: 0.9804\n",
      "Epoch 68/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1647 - acc: 0.9429 - val_loss: 0.0557 - val_acc: 0.9804\n",
      "Epoch 69/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1523 - acc: 0.9411 - val_loss: 0.0557 - val_acc: 0.9804\n",
      "Epoch 70/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1548 - acc: 0.9437 - val_loss: 0.0594 - val_acc: 0.9817\n",
      "Epoch 71/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1693 - acc: 0.9398 - val_loss: 0.0526 - val_acc: 0.9791\n",
      "Epoch 72/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1639 - acc: 0.9385 - val_loss: 0.0535 - val_acc: 0.9804\n",
      "Epoch 73/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1615 - acc: 0.9407 - val_loss: 0.0533 - val_acc: 0.9817\n",
      "Epoch 74/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1603 - acc: 0.9451 - val_loss: 0.0529 - val_acc: 0.9830\n",
      "Epoch 75/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1590 - acc: 0.9385 - val_loss: 0.0503 - val_acc: 0.9856\n",
      "Epoch 76/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1487 - acc: 0.9503 - val_loss: 0.0493 - val_acc: 0.9804\n",
      "Epoch 77/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1590 - acc: 0.9420 - val_loss: 0.0494 - val_acc: 0.9856\n",
      "Epoch 78/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1460 - acc: 0.9455 - val_loss: 0.0482 - val_acc: 0.9804\n",
      "Epoch 79/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1376 - acc: 0.9564 - val_loss: 0.0489 - val_acc: 0.9856\n",
      "Epoch 80/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1590 - acc: 0.9455 - val_loss: 0.0480 - val_acc: 0.9804\n",
      "Epoch 81/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1449 - acc: 0.9498 - val_loss: 0.0467 - val_acc: 0.9856\n",
      "Epoch 82/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1544 - acc: 0.9451 - val_loss: 0.0466 - val_acc: 0.9869\n",
      "Epoch 83/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1600 - acc: 0.9481 - val_loss: 0.0454 - val_acc: 0.9856\n",
      "Epoch 84/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1583 - acc: 0.9424 - val_loss: 0.0449 - val_acc: 0.9869\n",
      "Epoch 85/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1653 - acc: 0.9394 - val_loss: 0.0452 - val_acc: 0.9856\n",
      "Epoch 86/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1541 - acc: 0.9424 - val_loss: 0.0463 - val_acc: 0.9856\n",
      "Epoch 87/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1614 - acc: 0.9407 - val_loss: 0.0441 - val_acc: 0.9843\n",
      "Epoch 88/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1361 - acc: 0.9520 - val_loss: 0.0487 - val_acc: 0.9856\n",
      "Epoch 89/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1383 - acc: 0.9551 - val_loss: 0.0440 - val_acc: 0.9882\n",
      "Epoch 90/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1464 - acc: 0.9477 - val_loss: 0.0445 - val_acc: 0.9856\n",
      "Epoch 91/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1458 - acc: 0.9520 - val_loss: 0.0431 - val_acc: 0.9882\n",
      "Epoch 92/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1438 - acc: 0.9481 - val_loss: 0.0416 - val_acc: 0.9882\n",
      "Epoch 93/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1556 - acc: 0.9472 - val_loss: 0.0439 - val_acc: 0.9856\n",
      "Epoch 94/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1422 - acc: 0.9494 - val_loss: 0.0406 - val_acc: 0.9882\n",
      "Epoch 95/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1448 - acc: 0.9437 - val_loss: 0.0399 - val_acc: 0.9908\n",
      "Epoch 96/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1352 - acc: 0.9546 - val_loss: 0.0396 - val_acc: 0.9882\n",
      "Epoch 97/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1486 - acc: 0.9464 - val_loss: 0.0410 - val_acc: 0.9843\n",
      "Epoch 98/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1529 - acc: 0.9477 - val_loss: 0.0424 - val_acc: 0.9856\n",
      "Epoch 99/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1614 - acc: 0.9433 - val_loss: 0.0432 - val_acc: 0.9856\n",
      "Epoch 100/100\n",
      "2293/2293 [==============================] - 0s - loss: 0.1450 - acc: 0.9472 - val_loss: 0.0411 - val_acc: 0.9843\n",
      " 128/1427 [=>............................] - ETA: 0s[0.15984853122940715, 0.9670637701471618]\n"
     ]
    }
   ],
   "source": [
    "NeuralNetwork(train_in, train[:,-1], test_in, test[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
